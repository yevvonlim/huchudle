### 2024-04-12 EST 17:00 [Yoojeong]
- What I did today

    * Yoojeong had a mid-term presentation rehearsal, and got feedback about intonation and slide deck.
    * Yoojeong prepared for mid-term presentation rehearsal, reviewed testing code, VAE, and the testing result. 
    * Yoojeong studied how Variational Auto-Encoder(VAE) logliklihood is calculated.
* Variational Auto-Encoder(VAE) logliklihood explanation
* more explanation 

$$ \log p_θ(x) = E_{z}\left[ \frac{p_θ(x|z)p_θ(z)}{p_θ(z|x)} \right] 
$$

$$ 
= E_{z}\left[ \frac{p_θ(x|z)p_θ(z)}{p_θ(z|x)} \cdot \frac{q_φ(z|x)}{q_φ(z|x)} \right] 
$$

$$ 
= E_{z} [\log p_θ(x|z)] - E_{z}\left[\log \frac{q_φ(z|x)}{p(z)}\right] + E_z \left [\log \frac{q_φ(z|x)}{p_θ(z|x)}\right]
$$

$$ 
\log p_θ(x) = E_{z \sim q_{\phi}(z|x)} [\log p_θ(x)] - D_{KL}(q_{\phi}(z|x) || p(z)) + D_{KL}(q_{\phi}(z|x) || p_θ(z|x))
$$


### 2024-04-12 EST 17:00 [Jaeho]

- What I did today
    * Jaeho discussed DiT in today weekly meeting. The presentation emphasized DiT's patchify and transformer decoder function.

- What I learn today
    * In patching, an image is divided into smaller patches, typically squares, each with a part of the original image. These patches are then flattened into sequences of vectors, which then go into the transformer model.

### 2024-04-12 EST 17:00  [Hailey]

- What I did today
  * Hailey prepared for presentation. 
  
- What I learned today
  * Hailey learned about how latent space works in Stable Diffusion model.
  * By implementing latent space, the image generation task got higher scalabilty because of the computing efficiency.
  * Latent space got a text embedding vector as a input, and do denoising process by Unet structure, then generate a latent vector that fed into the decoder, which is the image generation part.

### 2024-04-12 EST 17:00 [Haeyeon Kim]
  
- What I did today 
  *  Haeyeon prerared for presentation. 

